{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining a Self-Explaining Neural Network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/SENN.png\" alt=\"SENN Architecture Diagram (Alvarez-Melis \\& Jaakkola)]\" style=\"width: 640px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Jacobian with Pytorch Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How Autograd builds a computation graph?**\n",
    "* Pytorch autograd engine is based on two major classes: ```Tensor``` and ```Function```\n",
    "* Using these together, the autograd engine builds a computation graph\n",
    "* Each ```Tensor``` has a ```.grad_fn``` attribute that records how it was generated\n",
    "* A user-defined ```Tensor``` (a leaf node on the computation graph) has its ```.grad_fn``` set to ```None```\n",
    "* A ```Tensor``` generated from an operation like ```+``` or ```*``` has its ```.grad_fn``` set to that ```Function``` operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to compute gradients in the Autograd computation graph?**\n",
    "* If the ```Tensor``` we want to compute the gradient against is a scalar, we simply call ```.backward()``` on it\n",
    "* If the ```Tensor``` we want to compute the gradient against is multi-dimensional, we pass a gradient value to the ```.backward()``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian_demo(in_dim, out_dim):\n",
    "    '''\n",
    "    A Jacobian Demo:\n",
    "    Set out_dim = 1 for default behaviour\n",
    "    Set out_dim > 1 for magic\n",
    "    '''\n",
    "    x = torch.ones(in_dim, requires_grad=True)\n",
    "    w = torch.randn((in_dim, out_dim))\n",
    "    y = x@w\n",
    "    print(f\"x= {x}\")\n",
    "    print(f\"W= {w}\")\n",
    "    print(f\"y= {y}\")\n",
    "\n",
    "    if out_dim == 1:\n",
    "        y.backward() # equivalent to y.backward(torch.tensor(1.))\n",
    "    else:\n",
    "        y.backward(torch.ones(out_dim))\n",
    "\n",
    "    print(f\"dy/dx = {x.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= tensor([1., 1., 1.], requires_grad=True)\n",
      "W= tensor([[1.7266],\n",
      "        [0.2947],\n",
      "        [0.8249]])\n",
      "y= tensor([2.8462], grad_fn=<SqueezeBackward3>)\n",
      "dy/dx = tensor([1.7266, 0.2947, 0.8249])\n"
     ]
    }
   ],
   "source": [
    "jacobian_demo(in_dim=3, out_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= tensor([1., 1., 1.], requires_grad=True)\n",
      "W= tensor([[ 0.4594, -0.0022],\n",
      "        [ 0.1305,  1.7321],\n",
      "        [-1.6384,  0.9940]])\n",
      "y= tensor([-1.0485,  2.7239], grad_fn=<SqueezeBackward3>)\n",
      "dy/dx = tensor([ 0.4572,  1.8626, -0.6444])\n"
     ]
    }
   ],
   "source": [
    "jacobian_demo(in_dim=3, out_dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch Autograd behind the scenes**\n",
    "* Every node in the computation graph recieves the gradient from the node above it\n",
    "* Autograd computes the product of the Jacobian of the current node with the incoming gradient vector\n",
    "* So the grad of a node is computed as: $\\mathbf{v}^{T} \\cdot \\mathbf{J}$\n",
    "* For a scalar node, the incoming gradient vector is just a ```1 x 1``` vector\n",
    "* The nodes after a scalar node then recieve the Jacobian of the scalar node w.r.t. to a vector as a ```out_dim x 1``` vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = 1\n",
    "num_features = 3\n",
    "num_outputs = 2\n",
    "x = torch.randn(batch, num_features)\n",
    "print(x.shape)\n",
    "w = nn.Parameter(torch.randn(num_features, num_outputs))\n",
    "net = lambda x: x@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = x.squeeze()\n",
    "n = x.size()[0]\n",
    "x = x.repeat(num_outputs, 1)\n",
    "x.requires_grad_(True)\n",
    "print(x.shape)\n",
    "y = net(x)\n",
    "print(y.shape)\n",
    "# step inside this call to see\n",
    "y.backward(torch.eye(num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian(f, x, out_dim):\n",
    "    input = x.clone().detach()\n",
    "    bsize = input.size()[0]\n",
    "    # (bs, in_dim) --repeated--> (bs, out_dim, in_dim)\n",
    "    input = input.unsqueeze(1).repeat(1, out_dim, 1)\n",
    "    input.requires_grad_(True)\n",
    "    # can only compute Jacobian of inputs and outputs with 2 dimensions\n",
    "    out = f(input).reshape(bsize, out_dim, out_dim)\n",
    "    # for autograd of non-scalar outputs\n",
    "    grad_matrix = torch.eye(out_dim).reshape(1,out_dim, out_dim).repeat(bsize, 1, 1)\n",
    "    out.backward(grad_matrix)\n",
    "    return input.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.relu(self.fc1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "batch = 2\n",
    "num_features = 5\n",
    "num_outputs = 2\n",
    "x = torch.randn(batch, num_features)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Net(num_features, num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.1231,  0.4033, -0.0072, -0.3862,  0.3884]],\n",
       " \n",
       "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.1231,  0.4033, -0.0072, -0.3862,  0.3884]]]),\n",
       " torch.Size([2, 2, 5]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J = jacobian(f, x, num_outputs)\n",
    "J, J.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENN as Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsize = 5\n",
    "in_dim = 3\n",
    "h_dim = in_dim # for the identity conceptizer case\n",
    "out_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3792,  0.1336,  0.4417],\n",
       "        [ 0.4731,  0.6780,  0.3843],\n",
       "        [ 1.5343,  0.2343, -0.1518],\n",
       "        [ 2.6796, -0.9424,  0.4785],\n",
       "        [-1.2728,  0.2543, -1.0110]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((bsize, in_dim))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor(bsize*[1], dtype=torch.float32)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conceptualizer = lambda x: x.unsqueeze(-1)\n",
    "h = Conceptualizer(x)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = nn.Parameter(torch.randn(in_dim, h_dim))\n",
    "Parameterizer = lambda x: (x@w).unsqueeze(-1) \n",
    "t = Parameterizer(x)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Aggregator = lambda x: (x[0].squeeze()*x[1].squeeze()).sum(1)\n",
    "g = Aggregator((t, x))\n",
    "g.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SENN(x):\n",
    "    h = Conceptualizer(x)\n",
    "    t = Parameterizer(x)\n",
    "    g = Aggregator((t,x))\n",
    "    return g, t, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "L_y = mse_loss(g, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Bound Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathcal{L}_\\theta := \\| \\nabla_{x} f(x) - \\theta(x)^{T} J_{x}^{h}(x) \\|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J_hx = jacobian(Conceptualizer, x, h_dim)\n",
    "J_hx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_SENN(x):\n",
    "    g, _, _ = SENN(x)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J_yx = jacobian(g_SENN, x, out_dim)\n",
    "J_yx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(t.permute(0,2,1), J_hx).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_t = (J_yx - torch.bmm(t.permute(0,2,1), J_hx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathcal{L}_y (f(x), y) + \\lambda \\mathcal{L}_\\theta (f) + \\xi \\mathcal{L}_h (x, \\hat{x})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = L_y + L_t.mean() + 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.8127, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENN with Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((bs, in_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([1,0,0,1,0,1,0,0,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conceptualizer(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameterizer(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn((in_dim, h_dim)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x@self.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn((h_dim, out_dim)))\n",
    "\n",
    "    def forward(self, concepts, relevances):\n",
    "        return (concepts + relevances)@self.W"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
