{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining a Self-Explaining Neural Network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/SENN.png\" alt=\"SENN Architecture Diagram (Alvarez-Melis \\& Jaakkola)]\" style=\"width: 640px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Jacobian with Pytorch Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How Autograd builds a computation graph?**\n",
    "* Pytorch autograd engine is based on two major classes: ```Tensor``` and ```Function```\n",
    "* Using these together, the autograd engine builds a computation graph\n",
    "* Each ```Tensor``` has a ```.grad_fn``` attribute that records how it was generated\n",
    "* A user-defined ```Tensor``` (a leaf node on the computation graph) has its ```.grad_fn``` set to ```None```\n",
    "* A ```Tensor``` generated from an operation like ```+``` or ```*``` has its ```.grad_fn``` set to that ```Function``` operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to compute gradients in the Autograd computation graph?**\n",
    "* If the ```Tensor``` we want to compute the gradient against is a scalar, we simply call ```.backward()``` on it\n",
    "* If the ```Tensor``` we want to compute the gradient against is multi-dimensional, we pass a gradient value to the ```.backward()``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian_demo(in_dim, out_dim):\n",
    "    '''\n",
    "    A Jacobian Demo:\n",
    "    Set out_dim = 1 for default behaviour\n",
    "    Set out_dim > 1 for magic\n",
    "    '''\n",
    "    x = torch.ones(in_dim, requires_grad=True)\n",
    "    w = torch.randn((in_dim, out_dim))\n",
    "    y = x@w\n",
    "    print(f\"x= {x}\")\n",
    "    print(f\"W= {w}\")\n",
    "    print(f\"y= {y}\")\n",
    "\n",
    "    if out_dim == 1:\n",
    "        y.backward() # equivalent to y.backward(torch.tensor(1.))\n",
    "    else:\n",
    "        y.backward(torch.ones(out_dim))\n",
    "\n",
    "    print(f\"dy/dx = {x.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= tensor([1., 1., 1.], requires_grad=True)\n",
      "W= tensor([[-0.1791],\n",
      "        [-1.3889],\n",
      "        [-0.8893]])\n",
      "y= tensor([-2.4573], grad_fn=<SqueezeBackward3>)\n",
      "dy/dx = tensor([-0.1791, -1.3889, -0.8893])\n"
     ]
    }
   ],
   "source": [
    "jacobian_demo(in_dim=3, out_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= tensor([1., 1., 1.], requires_grad=True)\n",
      "W= tensor([[-1.0334, -0.5454],\n",
      "        [-0.0359,  1.4675],\n",
      "        [-0.4654,  1.1262]])\n",
      "y= tensor([-1.5346,  2.0484], grad_fn=<SqueezeBackward3>)\n",
      "dy/dx = tensor([-1.5788,  1.4317,  0.6609])\n"
     ]
    }
   ],
   "source": [
    "jacobian_demo(in_dim=3, out_dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch Autograd behind the scenes**\n",
    "* Every node in the computation graph recieves the gradient from the node above it\n",
    "* Autograd computes the product of the Jacobian of the current node with the incoming gradient vector\n",
    "* So the grad of a node is computed as: $\\mathbf{v}^{T} \\cdot \\mathbf{J}$\n",
    "* For a scalar node, the incoming gradient vector is just a ```1 x 1``` vector\n",
    "* The nodes after a scalar node then recieve the Jacobian of the scalar node w.r.t. to a vector as a ```out_dim x 1``` vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = 1\n",
    "num_features = 3\n",
    "num_outputs = 2\n",
    "x = torch.randn(batch, num_features)\n",
    "print(x.shape)\n",
    "w = nn.Parameter(torch.randn(num_features, num_outputs))\n",
    "net = lambda x: x@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = x.squeeze()\n",
    "n = x.size()[0]\n",
    "x = x.repeat(num_outputs, 1)\n",
    "x.requires_grad_(True)\n",
    "print(x.shape)\n",
    "y = net(x)\n",
    "print(y.shape)\n",
    "# step inside this call to see\n",
    "y.backward(torch.eye(num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian(f, x, out_dim):\n",
    "    input = x.clone().detach()\n",
    "    bsize = input.size()[0]\n",
    "    # (bs, in_dim) --repeated--> (bs, out_dim, in_dim)\n",
    "    input = input.unsqueeze(1).repeat(1, out_dim, 1)\n",
    "    input.requires_grad_(True)\n",
    "    # can only compute Jacobian of inputs and outputs with 2 dimensions\n",
    "    out = f(input).reshape(bsize, out_dim, out_dim)\n",
    "    # for autograd of non-scalar outputs\n",
    "    grad_matrix = torch.eye(out_dim).reshape(1,out_dim, out_dim).repeat(bsize, 1, 1)\n",
    "    out.backward(grad_matrix)\n",
    "    return input.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.relu(self.fc1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "batch = 2\n",
    "num_features = 5\n",
    "num_outputs = 2\n",
    "x = torch.randn(batch, num_features)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Net(num_features, num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.4382, -0.1372, -0.3710,  0.1570, -0.1364],\n",
       "          [-0.3197,  0.0465, -0.1699,  0.3704, -0.1914]]]),\n",
       " torch.Size([2, 2, 5]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J = jacobian(f, x, num_outputs)\n",
    "J, J.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENN as Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsize = 10\n",
    "in_dim = 1\n",
    "h_dim = in_dim # for the identity conceptizer case\n",
    "out_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sine(bsize):\n",
    "    x = torch.arange(0, 2*np.pi, 2*np.pi/bsize).unsqueeze(-1)\n",
    "    y = torch.sin(x)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line(bsize):\n",
    "    x = torch.arange(0, 10, 10/bsize).unsqueeze(-1)\n",
    "    y = 2*x\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,y = get_line(bsize)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conceptualizer = lambda x: x\n",
    "concepts = Conceptualizer(x)\n",
    "concepts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = nn.Parameter(torch.randn(in_dim, out_dim*h_dim))\n",
    "Parameterizer = lambda x: (x@w) \n",
    "relevances = Parameterizer(x)\n",
    "relevances.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Aggregator = lambda x: (x[0]*x[1]).sum(1).unsqueeze(-1)\n",
    "aggregates = Aggregator((relevances, concepts))\n",
    "aggregates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conceptualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conceptualizer(nn.Module):\n",
    "    \n",
    "    def __init__(self):   super().__init__()\n",
    "    def forward(self, x): return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameterizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameterizer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, h_dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_dim, h_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.W(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, concepts, relevances):\n",
    "        return (concepts + relevances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SENN(nn.Module):\n",
    "    \n",
    "    def __init__(self, conceptualizer, parameterizer, aggregator):\n",
    "        super().__init__()\n",
    "        self.conceptualizer = conceptualizer\n",
    "        self.parameterizer = parameterizer\n",
    "        self.aggregator = aggregator\n",
    "    \n",
    "    def forward(self, x):\n",
    "        concepts = self.conceptualizer(x)\n",
    "        relevances = self.parameterizer(x)\n",
    "#         pdb.set_trace()\n",
    "        aggregates = self.aggregator(relevances,concepts)\n",
    "        return aggregates, relevances, concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathcal{L}_\\theta := \\| \\nabla_{x} f(x) - \\theta(x)^{T} J_{x}^{h}(x) \\|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_robustness_loss(x, relevances, senn):\n",
    "    '''Robustness Loss formulation for Linear Regression Task'''\n",
    "    # for linear regression\n",
    "    # both num_classes (out_dim) and num_concepts (h_dim) is 1\n",
    "    num_classes = relevances.size()[1]\n",
    "    num_concepts = relevances.size()[1]\n",
    "    relevances = relevances.reshape(-1,num_classes,num_concepts)\n",
    "    \n",
    "    def senn_aggregator(x): return senn(x)[0]\n",
    "    def senn_conceptualizer(x): return senn.conceptualizer(x)\n",
    "    \n",
    "    J_yx = jacobian(senn_aggregator, x, num_classes)\n",
    "    J_hx = jacobian(senn_conceptualizer, x, num_concepts)\n",
    "    robustness_loss = (J_yx - torch.bmm(relevances, J_hx))\n",
    "\n",
    "    return robustness_loss.norm(dim=(1,2)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_loss = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss=** $\\mathcal{L}_y (f(x), y) + \\lambda \\mathcal{L}_\\theta (f) + \\xi \\mathcal{L}_h (x, \\hat{x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 350\n",
    "num_features = in_dim\n",
    "num_classes = out_dim\n",
    "num_concepts = h_dim\n",
    "robust_lambda = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptualizer = Conceptualizer()\n",
    "parameterizer = Parameterizer(num_features, num_concepts)\n",
    "aggregator = Aggregator()\n",
    "senn = SENN(conceptualizer, parameterizer, aggregator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    aggregates, relevances, concepts = senn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \t[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "concepts: \t[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "relevances: \t[-0.8749522 -1.2409383 -1.6069244 -1.9729105 -2.3388968 -2.7048826\n",
      " -3.070869  -3.4368548 -3.8028412 -4.168827 ]\n",
      "aggregates: \t[-0.8749522 -0.2409383  0.3930756  1.0270895  1.6611032  2.2951174\n",
      "  2.929131   3.5631452  4.197159   4.831173 ]\n",
      "y: \t[ 0.  2.  4.  6.  8. 10. 12. 14. 16. 18.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"x: \\t{x.squeeze().numpy()}\")\n",
    "print(f\"concepts: \\t{concepts.squeeze().numpy()}\")\n",
    "print(f\"relevances: \\t{relevances.squeeze().numpy()}\")\n",
    "print(f\"aggregates: \\t{aggregates.squeeze().numpy()}\")\n",
    "print(f\"y: \\t{y.squeeze().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3660]], requires_grad=True)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senn.parameterizer.W.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(senn.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression:64.701 \n",
      "Regression:63.948 \n",
      "Regression:63.204 \n",
      "Regression:62.469 \n",
      "Regression:61.743 \n",
      "Regression:61.025 \n",
      "Regression:60.315 \n",
      "Regression:59.613 \n",
      "Regression:58.920 \n",
      "Regression:58.235 \n",
      "Regression:57.558 \n",
      "Regression:56.888 \n",
      "Regression:56.227 \n",
      "Regression:55.573 \n",
      "Regression:54.927 \n",
      "Regression:54.288 \n",
      "Regression:53.657 \n",
      "Regression:53.033 \n",
      "Regression:52.416 \n",
      "Regression:51.807 \n",
      "Regression:51.204 \n",
      "Regression:50.609 \n",
      "Regression:50.021 \n",
      "Regression:49.439 \n",
      "Regression:48.864 \n",
      "Regression:48.296 \n",
      "Regression:47.735 \n",
      "Regression:47.180 \n",
      "Regression:46.632 \n",
      "Regression:46.090 \n",
      "Regression:45.554 \n",
      "Regression:45.025 \n",
      "Regression:44.501 \n",
      "Regression:43.984 \n",
      "Regression:43.473 \n",
      "Regression:42.968 \n",
      "Regression:42.468 \n",
      "Regression:41.975 \n",
      "Regression:41.487 \n",
      "Regression:41.005 \n",
      "Regression:40.528 \n",
      "Regression:40.058 \n",
      "Regression:39.592 \n",
      "Regression:39.132 \n",
      "Regression:38.678 \n",
      "Regression:38.228 \n",
      "Regression:37.784 \n",
      "Regression:37.345 \n",
      "Regression:36.911 \n",
      "Regression:36.483 \n",
      "Regression:36.059 \n",
      "Regression:35.640 \n",
      "Regression:35.226 \n",
      "Regression:34.817 \n",
      "Regression:34.413 \n",
      "Regression:34.013 \n",
      "Regression:33.618 \n",
      "Regression:33.228 \n",
      "Regression:32.842 \n",
      "Regression:32.460 \n",
      "Regression:32.083 \n",
      "Regression:31.711 \n",
      "Regression:31.343 \n",
      "Regression:30.979 \n",
      "Regression:30.619 \n",
      "Regression:30.264 \n",
      "Regression:29.913 \n",
      "Regression:29.565 \n",
      "Regression:29.222 \n",
      "Regression:28.883 \n",
      "Regression:28.548 \n",
      "Regression:28.217 \n",
      "Regression:27.889 \n",
      "Regression:27.565 \n",
      "Regression:27.246 \n",
      "Regression:26.929 \n",
      "Regression:26.617 \n",
      "Regression:26.308 \n",
      "Regression:26.003 \n",
      "Regression:25.701 \n",
      "Regression:25.403 \n",
      "Regression:25.109 \n",
      "Regression:24.817 \n",
      "Regression:24.529 \n",
      "Regression:24.245 \n",
      "Regression:23.964 \n",
      "Regression:23.686 \n",
      "Regression:23.411 \n",
      "Regression:23.140 \n",
      "Regression:22.872 \n",
      "Regression:22.606 \n",
      "Regression:22.344 \n",
      "Regression:22.085 \n",
      "Regression:21.829 \n",
      "Regression:21.576 \n",
      "Regression:21.326 \n",
      "Regression:21.079 \n",
      "Regression:20.835 \n",
      "Regression:20.593 \n",
      "Regression:20.355 \n",
      "Regression:20.119 \n",
      "Regression:19.886 \n",
      "Regression:19.655 \n",
      "Regression:19.428 \n",
      "Regression:19.203 \n",
      "Regression:18.980 \n",
      "Regression:18.761 \n",
      "Regression:18.543 \n",
      "Regression:18.329 \n",
      "Regression:18.116 \n",
      "Regression:17.907 \n",
      "Regression:17.699 \n",
      "Regression:17.494 \n",
      "Regression:17.292 \n",
      "Regression:17.092 \n",
      "Regression:16.894 \n",
      "Regression:16.698 \n",
      "Regression:16.505 \n",
      "Regression:16.314 \n",
      "Regression:16.125 \n",
      "Regression:15.939 \n",
      "Regression:15.754 \n",
      "Regression:15.572 \n",
      "Regression:15.392 \n",
      "Regression:15.214 \n",
      "Regression:15.038 \n",
      "Regression:14.864 \n",
      "Regression:14.692 \n",
      "Regression:14.523 \n",
      "Regression:14.355 \n",
      "Regression:14.189 \n",
      "Regression:14.025 \n",
      "Regression:13.863 \n",
      "Regression:13.703 \n",
      "Regression:13.544 \n",
      "Regression:13.388 \n",
      "Regression:13.233 \n",
      "Regression:13.080 \n",
      "Regression:12.929 \n",
      "Regression:12.780 \n",
      "Regression:12.632 \n",
      "Regression:12.486 \n",
      "Regression:12.342 \n",
      "Regression:12.200 \n",
      "Regression:12.059 \n",
      "Regression:11.920 \n",
      "Regression:11.782 \n",
      "Regression:11.646 \n",
      "Regression:11.512 \n",
      "Regression:11.379 \n",
      "Regression:11.248 \n",
      "Regression:11.118 \n",
      "Regression:10.990 \n",
      "Regression:10.863 \n",
      "Regression:10.738 \n",
      "Regression:10.614 \n",
      "Regression:10.492 \n",
      "Regression:10.371 \n",
      "Regression:10.251 \n",
      "Regression:10.133 \n",
      "Regression:10.017 \n",
      "Regression:9.901 \n",
      "Regression:9.787 \n",
      "Regression:9.675 \n",
      "Regression:9.563 \n",
      "Regression:9.453 \n",
      "Regression:9.344 \n",
      "Regression:9.237 \n",
      "Regression:9.130 \n",
      "Regression:9.025 \n",
      "Regression:8.922 \n",
      "Regression:8.819 \n",
      "Regression:8.717 \n",
      "Regression:8.617 \n",
      "Regression:8.518 \n",
      "Regression:8.420 \n",
      "Regression:8.323 \n",
      "Regression:8.228 \n",
      "Regression:8.133 \n",
      "Regression:8.040 \n",
      "Regression:7.948 \n",
      "Regression:7.856 \n",
      "Regression:7.766 \n",
      "Regression:7.677 \n",
      "Regression:7.589 \n",
      "Regression:7.502 \n",
      "Regression:7.416 \n",
      "Regression:7.331 \n",
      "Regression:7.246 \n",
      "Regression:7.163 \n",
      "Regression:7.081 \n",
      "Regression:7.000 \n",
      "Regression:6.920 \n",
      "Regression:6.841 \n",
      "Regression:6.762 \n",
      "Regression:6.685 \n",
      "Regression:6.608 \n",
      "Regression:6.533 \n",
      "Regression:6.458 \n",
      "Regression:6.384 \n",
      "Regression:6.311 \n",
      "Regression:6.239 \n",
      "Regression:6.167 \n",
      "Regression:6.097 \n",
      "Regression:6.027 \n",
      "Regression:5.958 \n",
      "Regression:5.890 \n",
      "Regression:5.823 \n",
      "Regression:5.756 \n",
      "Regression:5.690 \n",
      "Regression:5.625 \n",
      "Regression:5.561 \n",
      "Regression:5.498 \n",
      "Regression:5.435 \n",
      "Regression:5.373 \n",
      "Regression:5.312 \n",
      "Regression:5.251 \n",
      "Regression:5.191 \n",
      "Regression:5.132 \n",
      "Regression:5.074 \n",
      "Regression:5.016 \n",
      "Regression:4.959 \n",
      "Regression:4.902 \n",
      "Regression:4.847 \n",
      "Regression:4.792 \n",
      "Regression:4.737 \n",
      "Regression:4.683 \n",
      "Regression:4.630 \n",
      "Regression:4.577 \n",
      "Regression:4.525 \n",
      "Regression:4.474 \n",
      "Regression:4.423 \n",
      "Regression:4.373 \n",
      "Regression:4.323 \n",
      "Regression:4.274 \n",
      "Regression:4.226 \n",
      "Regression:4.178 \n",
      "Regression:4.131 \n",
      "Regression:4.084 \n",
      "Regression:4.037 \n",
      "Regression:3.992 \n",
      "Regression:3.947 \n",
      "Regression:3.902 \n",
      "Regression:3.858 \n",
      "Regression:3.814 \n",
      "Regression:3.771 \n",
      "Regression:3.728 \n",
      "Regression:3.686 \n",
      "Regression:3.645 \n",
      "Regression:3.604 \n",
      "Regression:3.563 \n",
      "Regression:3.523 \n",
      "Regression:3.483 \n",
      "Regression:3.444 \n",
      "Regression:3.405 \n",
      "Regression:3.367 \n",
      "Regression:3.329 \n",
      "Regression:3.291 \n",
      "Regression:3.254 \n",
      "Regression:3.218 \n",
      "Regression:3.182 \n",
      "Regression:3.146 \n",
      "Regression:3.110 \n",
      "Regression:3.076 \n",
      "Regression:3.041 \n",
      "Regression:3.007 \n",
      "Regression:2.973 \n",
      "Regression:2.940 \n",
      "Regression:2.907 \n",
      "Regression:2.874 \n",
      "Regression:2.842 \n",
      "Regression:2.810 \n",
      "Regression:2.779 \n",
      "Regression:2.748 \n",
      "Regression:2.717 \n",
      "Regression:2.687 \n",
      "Regression:2.657 \n",
      "Regression:2.627 \n",
      "Regression:2.598 \n",
      "Regression:2.569 \n",
      "Regression:2.541 \n",
      "Regression:2.512 \n",
      "Regression:2.484 \n",
      "Regression:2.457 \n",
      "Regression:2.429 \n",
      "Regression:2.402 \n",
      "Regression:2.376 \n",
      "Regression:2.349 \n",
      "Regression:2.323 \n",
      "Regression:2.298 \n",
      "Regression:2.272 \n",
      "Regression:2.247 \n",
      "Regression:2.222 \n",
      "Regression:2.198 \n",
      "Regression:2.173 \n",
      "Regression:2.149 \n",
      "Regression:2.126 \n",
      "Regression:2.102 \n",
      "Regression:2.079 \n",
      "Regression:2.056 \n",
      "Regression:2.033 \n",
      "Regression:2.011 \n",
      "Regression:1.989 \n",
      "Regression:1.967 \n",
      "Regression:1.945 \n",
      "Regression:1.924 \n",
      "Regression:1.903 \n",
      "Regression:1.882 \n",
      "Regression:1.862 \n",
      "Regression:1.841 \n",
      "Regression:1.821 \n",
      "Regression:1.801 \n",
      "Regression:1.782 \n",
      "Regression:1.762 \n",
      "Regression:1.743 \n",
      "Regression:1.724 \n",
      "Regression:1.705 \n",
      "Regression:1.687 \n",
      "Regression:1.668 \n",
      "Regression:1.650 \n",
      "Regression:1.632 \n",
      "Regression:1.615 \n",
      "Regression:1.597 \n",
      "Regression:1.580 \n",
      "Regression:1.563 \n",
      "Regression:1.546 \n",
      "Regression:1.529 \n",
      "Regression:1.513 \n",
      "Regression:1.496 \n",
      "Regression:1.480 \n",
      "Regression:1.464 \n",
      "Regression:1.449 \n",
      "Regression:1.433 \n",
      "Regression:1.418 \n",
      "Regression:1.402 \n",
      "Regression:1.387 \n",
      "Regression:1.373 \n",
      "Regression:1.358 \n",
      "Regression:1.343 \n",
      "Regression:1.329 \n",
      "Regression:1.315 \n",
      "Regression:1.301 \n",
      "Regression:1.287 \n",
      "Regression:1.273 \n",
      "Regression:1.260 \n",
      "Regression:1.246 \n",
      "Regression:1.233 \n",
      "Regression:1.220 \n",
      "Regression:1.207 \n",
      "Regression:1.195 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    opt.zero_grad()\n",
    "    aggregates, relevances, concepts = senn(x)\n",
    "#     pdb.set_trace()\n",
    "    regression_loss = mse_loss(aggregates, y) \n",
    "#     robustness_loss = regression_robustness_loss(x, relevances, senn)\n",
    "#     loss = regression_loss + robust_lambda*robustness_loss + recon_loss\n",
    "    \n",
    "#     print(f\"Loss:{loss.item():.3f} \"\n",
    "#           f\"Regression:{regression_loss.item():.3f} \"\n",
    "#           f\"Robustness: {robustness_loss.item():.3f}\")\n",
    "    regression_loss.backward()\n",
    "    print(f\"Regression:{regression_loss.item():.3f} \")\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9114202260971069"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senn.parameterizer.W.weight.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
