{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining a Self-Explaining Neural Network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/SENN.png\" alt=\"SENN Architecture Diagram (Alvarez-Melis \\& Jaakkola)]\" style=\"width: 640px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Jacobian with Pytorch Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How Autograd builds a computation graph?**\n",
    "* Pytorch autograd engine is based on two major classes: ```Tensor``` and ```Function```\n",
    "* Using these together, the autograd engine builds a computation graph\n",
    "* Each ```Tensor``` has a ```.grad_fn``` attribute that records how it was generated\n",
    "* A user-defined ```Tensor``` (a leaf node on the computation graph) has its ```.grad_fn``` set to ```None```\n",
    "* A ```Tensor``` generated from an operation like ```+``` or ```*``` has its ```.grad_fn``` set to that ```Function``` operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to compute gradients in the Autograd computation graph?**\n",
    "* If the ```Tensor``` we want to compute the gradient against is a scalar, we simply call ```.backward()``` on it\n",
    "* If the ```Tensor``` we want to compute the gradient against is multi-dimensional, we pass a gradient value to the ```.backward()``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian_demo(in_dim, out_dim):\n",
    "    '''\n",
    "    A Jacobian Demo:\n",
    "    Set out_dim = 1 for default behaviour\n",
    "    Set out_dim > 1 for magic\n",
    "    '''\n",
    "    x = torch.ones(in_dim, requires_grad=True)\n",
    "    w = torch.randn((in_dim, out_dim))\n",
    "    y = x@w\n",
    "    print(f\"x= {x}\")\n",
    "    print(f\"W= {w}\")\n",
    "    print(f\"y= {y}\")\n",
    "\n",
    "    if out_dim == 1:\n",
    "        y.backward() # equivalent to y.backward(torch.tensor(1.))\n",
    "    else:\n",
    "        y.backward(torch.ones(out_dim))\n",
    "\n",
    "    print(f\"dy/dx = {x.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= tensor([1., 1., 1.], requires_grad=True)\n",
      "W= tensor([[ 0.2518],\n",
      "        [ 1.2410],\n",
      "        [-0.0681]])\n",
      "y= tensor([1.4247], grad_fn=<SqueezeBackward3>)\n",
      "dy/dx = tensor([ 0.2518,  1.2410, -0.0681])\n"
     ]
    }
   ],
   "source": [
    "jacobian_demo(in_dim=3, out_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= tensor([1., 1., 1.], requires_grad=True)\n",
      "W= tensor([[-0.0038,  0.9529],\n",
      "        [ 1.4555,  0.3204],\n",
      "        [ 0.0197,  1.1247]])\n",
      "y= tensor([1.4714, 2.3980], grad_fn=<SqueezeBackward3>)\n",
      "dy/dx = tensor([0.9491, 1.7759, 1.1445])\n"
     ]
    }
   ],
   "source": [
    "jacobian_demo(in_dim=3, out_dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch Autograd behind the scenes**\n",
    "* Every node in the computation graph recieves the gradient from the node above it\n",
    "* Autograd computes the product of the Jacobian of the current node with the incoming gradient vector\n",
    "* So the grad of a node is computed as: $\\mathbf{v}^{T} \\cdot \\mathbf{J}$\n",
    "* For a scalar node, the incoming gradient vector is just a ```1 x 1``` vector\n",
    "* The nodes after a scalar node then recieve the Jacobian of the scalar node w.r.t. to a vector as a ```out_dim x 1``` vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = 1\n",
    "num_features = 3\n",
    "num_outputs = 2\n",
    "x = torch.randn(batch, num_features)\n",
    "print(x.shape)\n",
    "w = nn.Parameter(torch.randn(num_features, num_outputs))\n",
    "net = lambda x: x@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = x.squeeze()\n",
    "n = x.size()[0]\n",
    "x = x.repeat(num_outputs, 1)\n",
    "x.requires_grad_(True)\n",
    "print(x.shape)\n",
    "y = net(x)\n",
    "print(y.shape)\n",
    "y.backward(torch.eye(num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian(f, x, out_dim):\n",
    "    input = torch.tensor(x.clone().detach(), requires_grad=True)\n",
    "    bsize = input.size()[0]\n",
    "    # (bs, in_dim) --repeated--> (bs, out_dim, in_dim)\n",
    "    input = input.unsqueeze(1).repeat(1, out_dim, 1)\n",
    "    out = f(input)\n",
    "    # for autograd of non-scalar outputs\n",
    "    grad_matrix = torch.eye(out_dim).reshape(1,out_dim, out_dim).repeat(bsize, 1, 1)\n",
    "    out.backward(grad_matrix)\n",
    "    return input.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.relu(self.fc1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "batch = 1\n",
    "num_features = 5\n",
    "num_outputs = 2\n",
    "x = torch.randn(batch, num_features)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Net(num_features, num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = jacobian(f, x, num_outputs)\n",
    "J, J.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((bs, in_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([1,0,0,1,0,1,0,0,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conceptualizer(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameterizer(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn((in_dim, h_dim)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x@self.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn((h_dim, out_dim)))\n",
    "\n",
    "    def forward(self, concepts, relevances):\n",
    "        return (concepts + relevances)@self.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = Conceptualizer()\n",
    "t = Parameterizer()\n",
    "g = Aggregator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = h(x)\n",
    "relevances = t(x)\n",
    "y_pred = g(concepts, relevances) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENN as Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsize = 5\n",
    "in_dim = 3\n",
    "h_dim = in_dim # needs discussion\n",
    "out_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0170, -1.0168,  0.6083],\n",
       "        [ 0.2550,  1.6877,  0.5968],\n",
       "        [ 0.3455,  2.0961,  1.8915],\n",
       "        [-0.3178,  1.0152,  2.0694],\n",
       "        [ 1.0242,  0.3378,  1.0517]], requires_grad=True)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((bsize, in_dim))\n",
    "x.requires_grad_(True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor(bsize*[1], dtype=torch.float32)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conceptualizer = lambda x: x\n",
    "h = Conceptualizer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3459,  1.0733, -0.4981],\n",
       "        [-1.7992, -0.1439,  1.4994],\n",
       "        [-3.1064,  0.8840,  2.2621],\n",
       "        [-2.3626,  1.1694,  1.2552],\n",
       "        [-1.0963,  1.4317,  1.0703]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = nn.Parameter(torch.randn(in_dim, h_dim))\n",
    "Parameterizer = lambda x: x@w \n",
    "t = Parameterizer(x)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0312,  0.6309,  0.4697],\n",
       "        [-0.7936, -0.5017,  0.6987],\n",
       "        [-0.7571,  0.9081,  0.3359]], requires_grad=True)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.8851, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Aggregator = lambda x: (x[0]*x[1]).sum()\n",
    "g = Aggregator((t, x))\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "L_y = mse_loss(g, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Bound Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathcal{L}_\\theta := \\| \\nabla_{x} f(x) - \\theta(x)^{T} J_{x}^{h}(x) \\|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dummy = x.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_hx = jacobian(Conceptualizer, x_dummy, h_dim).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 3])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J_hx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0104,  1.9950, -1.2299],\n",
       "        [-0.4621, -0.7762,  3.0393],\n",
       "        [-0.9064,  0.8796,  4.5394],\n",
       "        [-0.7402,  2.3581,  3.1129],\n",
       "        [-0.4212,  1.1842,  0.9550]])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_t = (x.grad.data.squeeze() - t@torch.t(J_hx)).sum() # doesn't work for batches\n",
    "# need to fix the matrix mult\n",
    "# or change to denominator layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathcal{L}_y (f(x), y) + \\lambda \\mathcal{L}_\\theta (f) + \\xi \\mathcal{L}_h (x, \\hat{x})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
